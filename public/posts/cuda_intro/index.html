<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Starting out with CUDA C&#43;&#43; | Sunny Coding</title>
<meta name="keywords" content="CUDA, C&#43;&#43;, GPU, Parallelization">
<meta name="description" content="I&rsquo;ve been learning about CUDA C&#43;&#43; programming this week, following this blog post by Mark Harris.
What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially.
The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each.
I&rsquo;ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products.
You can see the full code in my repository here: https://github.com/davidnabergoj/cuda-programming.">
<meta name="author" content="David Nabergoj">
<link rel="canonical" href="https://nabergoj.org/posts/cuda_intro/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fd5566c526ae48aadabd950798a2dc3568536401560eae5caac3765a42a9e7b5.css" integrity="sha256-/VVmxSauSKravZUHmKLcNWhTZAFWDq5cqsN2WkKp57U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nabergoj.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nabergoj.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nabergoj.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nabergoj.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://nabergoj.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://nabergoj.org/posts/cuda_intro/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
<meta property="og:url" content="https://nabergoj.org/posts/cuda_intro/">
  <meta property="og:site_name" content="Sunny Coding">
  <meta property="og:title" content="Starting out with CUDA C&#43;&#43;">
  <meta property="og:description" content="I’ve been learning about CUDA C&#43;&#43; programming this week, following this blog post by Mark Harris. What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially. The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each. I’ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products. You can see the full code in my repository here: https://github.com/davidnabergoj/cuda-programming.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-12-05T00:00:00+00:00">
    <meta property="article:tag" content="CUDA">
    <meta property="article:tag" content="C&#43;&#43;">
    <meta property="article:tag" content="GPU">
    <meta property="article:tag" content="Parallelization">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Starting out with CUDA C&#43;&#43;">
<meta name="twitter:description" content="I&rsquo;ve been learning about CUDA C&#43;&#43; programming this week, following this blog post by Mark Harris.
What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially.
The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each.
I&rsquo;ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products.
You can see the full code in my repository here: https://github.com/davidnabergoj/cuda-programming.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nabergoj.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Starting out with CUDA C++",
      "item": "https://nabergoj.org/posts/cuda_intro/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Starting out with CUDA C++",
  "name": "Starting out with CUDA C\u002b\u002b",
  "description": "I\u0026rsquo;ve been learning about CUDA C++ programming this week, following this blog post by Mark Harris. What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially. The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each. I\u0026rsquo;ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products. You can see the full code in my repository here: https://github.com/davidnabergoj/cuda-programming.\n",
  "keywords": [
    "CUDA", "C++", "GPU", "Parallelization"
  ],
  "articleBody": "I’ve been learning about CUDA C++ programming this week, following this blog post by Mark Harris. What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially. The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each. I’ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products. You can see the full code in my repository here: https://github.com/davidnabergoj/cuda-programming.\nSequential addition The sequential approach to adding two vectors is straightforward. We simply iterate over all indices and add the values. x and y are pointers to two arrays of size n.\nvoid add(int n, float *x, float *y) { for (size_t i = 0; i \u003c n; i++) { y[i] = x[i] + y[i]; } } However, adding vectors this way only executes one addition at a time. Doing so in parallel would save us a lot of time, as we wouldn’t have to wait for previous additions to finish.\nParallel addition - single block CUDA kernels are an extension of C++ code which are executed on the GPU. We can change our previous function into a CUDA kernel by adding the __global__ keyword.\n__global__ void add(int n, float *x, float *y, float *sum) { int index = threadIdx.x; int stride = blockDim.x; for (size_t i = index; i \u003c n; i += stride) { sum[i] = x[i] + y[i]; } } Our CUDA kernel accesses two global variables: threadIdx.x and blockDim.x that describe which thread is used for this computation. When executing the kernel, we are essentially working with an array of threads called a block. We want each thread to compute x[i] + y[i] for indices i in its part of the array. We are essentially delegating work to different threads. We can imagine threads in a block to be like construction workers in a team.\nSuppose we have a block with four threads and \\(n = 11\\) elements in both our arrays. This makes blockDim.x = 4. The value of threadIdx.x will be one of 0, 1, 2, or 3. Let’s give each thread a color: orange for zero, blue for 1, green for 2, and purple for 3. For a given thread with index threadIdx.x, the for loop will go through indices i = threadIdx.x + 0, i = threadIdx.x + 4, i = threadIdx.x + 8, etc. The loop will stop once i goes beyond the bounds of the input arrays. At each index, the thread will compute and store the sum of the two array elements.\nSince the threads are executed in parallel, each for loop will only take three iterations. The purple thread with threadIdx.x = 3 will be done two iterations, while others need three. This is in contrast to 11 iterations on the CPU program. In this case, the CUDA approach will theoretically only need \\(n / 4\\) loop iterations. Increasing the number of threads makes this even faster! With \\(m\\) threads, we only need \\(n / m\\) iterations. This is great for very large vectors!\nParallel addition - multiple blocks We can take our parallelization one step further by using multiple blocks in parallel. These blocks are arranged in a grid. Using our analogy from before, multiple blocks are like multiple teams of construction workers. The grid is like a construction site with multiple teams, each working on their own separate task.\n__global__ void add(int n, float *x, float *y, float *sum) { int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = blockDim.x * gridDim.x; for (size_t i = index; i \u003c n; i += stride) { sum[i] = x[i] + y[i]; } } The kernel code stays largely the same. The only difference is figuring out which parts of the array our thread should process. The initial index for each thread is threadIdx.x - local to its block, offset by the total number of threads in all blocks before it. The stride was previously equal to the number of threads in the block, meaning the for loop increments the index by the block size. However, the new way of indexing requires that we increment the index by the sum of all block sizes.\nYou can check out the picture below for a visualization. Solid lines denote threads in block 1, while dashed lines denote threads in block 2. I’m not showing the actual values of x and y, as there are too many :P\nIf we have \\(B\\) blocks with \\(m\\) threads each, we now only require \\(n / (Bm)\\) for loop steps! This makes parallel execution even faster.\nComputing the dot product Let’s look at another example: computing the dot product of two vectors. Mathematically, the dot product is defined as \\(a^\\top b = \\sum_{i = 1}^n a_i b_i\\). Translating this to C++ means looping over the elements and adding the products a[i] * b[i] to a result out. In the code below, d is a pointer to a float.\nvoid dot(float *a, float *b, float *d, size_t n) { float out = 0.0; for (size_t i = 0; i \u003c n; i++) { out += a[i] * b[i]; } *d = out; } How can we make this faster? We tell each block to compute a partial sum. The mathematical idea is \\(a^\\top b = \\sum_{i = 1}^n a_i b_i = \\sum_{j = 1}^k \\sum_{i = 1}^m a_{jk+i} b_{jk+i} \\) where \\(j\\) is an index over \\(k\\) blocks, and \\(i\\) is an index over \\(m\\) threads within each block.\n#define BLOCK_SIZE 32 __global__ void dot_psum(float* a, float* b, float* p, size_t n) { __shared__ float sdata[BLOCK_SIZE]; // the whole block can access this size_t tid = threadIdx.x; size_t idx = blockIdx.x * BLOCK_SIZE + threadIdx.x; sdata[tid] = (idx \u003c n) ? a[idx] * b[idx] : 0; __syncthreads(); // wait until all threads in this block have written to sdata for (size_t s = BLOCK_SIZE / 2; s \u003e 0; s \u003e\u003e= 1) { if (tid \u003c s) { sdata[tid] += sdata[tid + s]; } __syncthreads(); } // Only allow thread 0 to write, otherwise we get race conditions and undefined behavior if (tid == 0) { p[blockIdx.x] = sdata[0]; } } In the kernel above, p is a pointer to a float array which holds \\( k \\) partial sums. We use an array called sdata with size \\(m\\) that is shared across all threads in a block. Each thread writes its own product to sdata. We use __syncthreads() to wait until all threads have successfully written to sdata.\nWe want to then sum all elements in sdata to create the partial sum. We use a clever strategy which only needs \\(O(\\log_2 m)\\) parallel iterations. In each thread, we use a for loop to produce different offsets s. If the thread index tid is less than the offset s, we look at the numbers in sdata[tid] and its offset counterpart sdata[tid + s]. We add sdata[tid + s] to sdata[tid]. After each loop step, we no longer have to look at sdata[tid + s], as its value has been accumulated into sdata[tid].\nCheck out the visualization for the first step, where we’re pretending to have a block of size \\(m = 8\\). The accumulation is only performed by threads 0, 1, 2, and 3, because threads 4, 5, 6, 7 have index greater than s = 4.\nAfterward, we apply the reduction again.\nAnd again :)\nNow the entire sum is located in the first element of sdata and we can write it to the output array. We’ll tell thread 0 of the block to do it, as having more than one thread trying to write to the same value will result in problems.\nif (tid == 0) { p[blockIdx.x] = sdata[0]; } Once all blocks have done this, the array of partial sums p is full. What remains is to sum the partial sums. A simple way of doing so is transferring the result back to the CPU and summing them sequentially.\nfloat result = 0.0f; for (size_t i = 0; i \u003c blocks; i++) { result += p[i]; } However, we can take it a step further and use a CUDA kernel to apply the final summation. The code is very similar! The only difference is we do not multiply two values when construcing sdata, but instead simply copy them over. In this kernel, we use the input array of partial sums in as the output as well, effectively modifying it in place. At the end, the result is stored in p[0].\n__global__ void reduce_sum(float* in, size_t n) { size_t tid = threadIdx.x; __shared__ float sdata[BLOCK_SIZE]; size_t idx = blockIdx.x * BLOCK_SIZE + threadIdx.x; sdata[tid] = (idx \u003c n) ? in[idx] : 0; __syncthreads(); for (size_t s = BLOCK_SIZE / 2; s \u003e 0; s \u003e\u003e= 1) { if (tid \u003c s) { sdata[tid] += sdata[tid + s]; } __syncthreads(); } if (tid == 0) { in[blockIdx.x] = sdata[0]; } } Thanks for reading! If you liked this post, you can support me on Ko-fi ☕. More CUDA coming soon :)\n",
  "wordCount" : "1525",
  "inLanguage": "en",
  "datePublished": "2025-12-05T00:00:00Z",
  "dateModified": "2025-12-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "David Nabergoj"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nabergoj.org/posts/cuda_intro/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sunny Coding",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nabergoj.org/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nabergoj.org/" accesskey="h" title="Sunny Coding (Alt + H)">Sunny Coding</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://nabergoj.org/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://nabergoj.org/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://nabergoj.org/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://nabergoj.org/">Home</a>&nbsp;»&nbsp;<a href="https://nabergoj.org/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Starting out with CUDA C&#43;&#43;
    </h1>
    <div class="post-meta"><span title='2025-12-05 00:00:00 +0000 UTC'>December 5, 2025</span>&nbsp;·&nbsp;<span>8 min</span>&nbsp;·&nbsp;<span>David Nabergoj</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#sequential-addition" aria-label="Sequential addition">Sequential addition</a></li>
                <li>
                    <a href="#parallel-addition---single-block" aria-label="Parallel addition - single block">Parallel addition - single block</a></li>
                <li>
                    <a href="#parallel-addition---multiple-blocks" aria-label="Parallel addition - multiple blocks">Parallel addition - multiple blocks</a></li>
                <li>
                    <a href="#computing-the-dot-product" aria-label="Computing the dot product">Computing the dot product</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>I&rsquo;ve been learning about CUDA C++ programming this week, following <a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">this blog post</a> by Mark Harris.
What makes CUDA useful is its ability to execute many computations in parallel instead of sequentially.
The linked blog post demonstrates how it can add two very large vectors with a size of 1 million elements each.
I&rsquo;ll present the most interesting code snippets in an approachable way and show how very similar code can perform vector-vector dot products.
You can see the full code in my repository here: <a href="https://github.com/davidnabergoj/cuda-programming">https://github.com/davidnabergoj/cuda-programming</a>.</p>
<h2 id="sequential-addition">Sequential addition<a hidden class="anchor" aria-hidden="true" href="#sequential-addition">#</a></h2>
<p>The sequential approach to adding two vectors is straightforward.
We simply iterate over all indices and add the values.
<code>x</code> and <code>y</code> are pointers to two arrays of size <code>n</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>However, adding vectors this way only executes one addition at a time.
Doing so in parallel would save us a lot of time, as we wouldn&rsquo;t have to wait for previous additions to finish.</p>
<h2 id="parallel-addition---single-block">Parallel addition - single block<a hidden class="anchor" aria-hidden="true" href="#parallel-addition---single-block">#</a></h2>
<p>CUDA kernels are an extension of C++ code which are executed on the GPU.
We can change our previous function into a CUDA kernel by adding the <code>__global__</code> keyword.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">sum</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">sum</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Our CUDA kernel accesses two global variables: <code>threadIdx.x</code> and <code>blockDim.x</code> that describe which thread is used for this computation.
When executing the kernel, we are essentially working with an array of threads called a block.
We want each thread to compute <code>x[i] + y[i]</code> for indices <code>i</code> in its part of the array.
We are essentially delegating work to different threads.
We can imagine threads in a block to be like construction workers in a team.</p>
<p>Suppose we have a block with four threads and \(n = 11\) elements in both our arrays.
This makes <code>blockDim.x = 4</code>. The value of <code>threadIdx.x</code> will be one of 0, 1, 2, or 3.
Let&rsquo;s give each thread a color: orange for zero, blue for 1, green for 2, and purple for 3.
For a given thread with index <code>threadIdx.x</code>, the for loop will go through indices <code>i = threadIdx.x + 0</code>, <code>i = threadIdx.x + 4</code>, <code>i = threadIdx.x + 8</code>, etc.
The loop will stop once <code>i</code> goes beyond the bounds of the input arrays.
At each index, the thread will compute and store the sum of the two array elements.</p>
<p><img alt="Parallel computation with four threads in a single block" loading="lazy" src="/cuda_intro/single_block.png#center"></p>
<p>Since the threads are executed in parallel, each for loop will only take three iterations.
The purple thread with <code>threadIdx.x = 3</code> will be done two iterations, while others need three.
This is in contrast to 11 iterations on the CPU program.
In this case, the CUDA approach will theoretically only need \(n / 4\) loop iterations.
Increasing the number of threads makes this even faster! With \(m\) threads, we only need \(n / m\) iterations.
This is great for very large vectors!</p>
<h2 id="parallel-addition---multiple-blocks">Parallel addition - multiple blocks<a hidden class="anchor" aria-hidden="true" href="#parallel-addition---multiple-blocks">#</a></h2>
<p>We can take our parallelization one step further by using multiple blocks in parallel.
These blocks are arranged in a grid.
Using our analogy from before, multiple blocks are like multiple teams of construction workers.
The grid is like a construction site with multiple teams, each working on their own separate task.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">sum</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">index</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">sum</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The kernel code stays largely the same. The only difference is figuring out which parts of the array our thread should process.
The initial index for each thread is <code>threadIdx.x</code> - local to its block, offset by the total number of threads in all blocks before it.
The stride was previously equal to the number of threads in the block, meaning the for loop increments the index by the block size.
However, the new way of indexing requires that we increment the index by the sum of all block sizes.</p>
<p>You can check out the picture below for a visualization. Solid lines denote threads in block 1, while dashed lines denote threads in block 2.
I&rsquo;m not showing the actual values of <code>x</code> and <code>y</code>, as there are too many :P</p>
<p><img alt="Parallel computation with two blocks, four threads per block" loading="lazy" src="/cuda_intro/multiple_blocks.png#center"></p>
<p>If we have \(B\) blocks with \(m\) threads each, we now only require \(n / (Bm)\) for loop steps! This makes parallel execution even faster.</p>
<h2 id="computing-the-dot-product">Computing the dot product<a hidden class="anchor" aria-hidden="true" href="#computing-the-dot-product">#</a></h2>
<p>Let&rsquo;s look at another example: computing the dot product of two vectors.
Mathematically, the dot product is defined as \(a^\top b = \sum_{i = 1}^n a_i b_i\).
Translating this to C++ means looping over the elements and adding the products <code>a[i] * b[i]</code> to a result <code>out</code>.
In the code below, <code>d</code> is a pointer to a float.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">dot</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">out</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="o">*</span><span class="n">d</span> <span class="o">=</span> <span class="n">out</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>How can we make this faster?
We tell each block to compute a partial sum.
The mathematical idea is \(a^\top b = \sum_{i = 1}^n a_i b_i = \sum_{j = 1}^k \sum_{i = 1}^m a_{jk+i} b_{jk+i} \) where \(j\) is an index over \(k\) blocks, and \(i\) is an index over \(m\) threads within each block.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define BLOCK_SIZE 32
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">dot_psum</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="kt">float</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="kt">float</span><span class="o">*</span> <span class="n">p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">sdata</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>  <span class="c1">// the whole block can access this
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">size_t</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">size_t</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">?</span> <span class="n">a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>  <span class="c1">// wait until all threads in this block have written to sdata
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">s</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Only allow thread 0 to write, otherwise we get race conditions and undefined behavior
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">sdata</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>In the kernel above, <code>p</code> is a pointer to a float array which holds \( k \) partial sums.
We use an array called <code>sdata</code> with size \(m\) that is shared across all threads in a block.
Each thread writes its own product to <code>sdata</code>.
We use <code>__syncthreads()</code> to wait until all threads have successfully written to <code>sdata</code>.</p>
<p>We want to then sum all elements in <code>sdata</code> to create the partial sum.
We use a clever strategy which only needs \(O(\log_2 m)\) parallel iterations.
In each thread, we use a for loop to produce different offsets <code>s</code>.
If the thread index <code>tid</code> is less than the offset <code>s</code>, we look at the numbers in <code>sdata[tid]</code> and its offset counterpart <code>sdata[tid + s]</code>.
We add <code>sdata[tid + s]</code> to <code>sdata[tid]</code>.
After each loop step, we no longer have to look at <code>sdata[tid + s]</code>, as its value has been accumulated into <code>sdata[tid]</code>.</p>
<p>Check out the visualization for the first step, where we&rsquo;re pretending to have a block of size \(m = 8\).
The accumulation is only performed by threads 0, 1, 2, and 3, because threads 4, 5, 6, 7 have index greater than <code>s = 4</code>.</p>
<p><img alt="Partial sum reduction in a single block - step 1" loading="lazy" src="/cuda_intro/partial_reduction_1.png#center"></p>
<p>Afterward, we apply the reduction again.</p>
<p><img alt="Partial sum reduction in a single block - step 2" loading="lazy" src="/cuda_intro/partial_reduction_2.png#center"></p>
<p>And again :)</p>
<p><img alt="Partial sum reduction in a single block - step 3" loading="lazy" src="/cuda_intro/partial_reduction_3.png#center"></p>
<p>Now the entire sum is located in the first element of <code>sdata</code> and we can write it to the output array.
We&rsquo;ll tell thread 0 of the block to do it, as having more than one thread trying to write to the same value will result in problems.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">sdata</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Once all blocks have done this, the array of partial sums <code>p</code> is full.
What remains is to <em>sum</em> the partial sums.
A simple way of doing so is transferring the result back to the CPU and summing them sequentially.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">blocks</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">+=</span> <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>However, we can take it a step further and use a CUDA kernel to apply the final summation.
The code is very similar!
The only difference is we do not multiply two values when construcing <code>sdata</code>, but instead simply copy them over.
In this kernel, we use the input array of partial sums <code>in</code> as the output as well, effectively modifying it in place.
At the end, the result is stored in <code>p[0]</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">in</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">size_t</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">sdata</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">size_t</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">?</span> <span class="n">in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">s</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">in</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">sdata</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Thanks for reading! If you liked this post, you can support me on <a href="https://ko-fi.com/davidnabergoj">Ko-fi ☕</a>. More CUDA coming soon :)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://nabergoj.org/tags/cuda/">CUDA</a></li>
      <li><a href="https://nabergoj.org/tags/c&#43;&#43;/">C&#43;&#43;</a></li>
      <li><a href="https://nabergoj.org/tags/gpu/">GPU</a></li>
      <li><a href="https://nabergoj.org/tags/parallelization/">Parallelization</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://nabergoj.org/posts/leetcode_1143/">
    <span class="title">Next »</span>
    <br>
    <span>Leetcode 1143: Longest Common Subsequence</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on x"
            href="https://x.com/intent/tweet/?text=Starting%20out%20with%20CUDA%20C%2b%2b&amp;url=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f&amp;hashtags=CUDA%2cC%2b%2b%2cGPU%2cParallelization">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f&amp;title=Starting%20out%20with%20CUDA%20C%2b%2b&amp;summary=Starting%20out%20with%20CUDA%20C%2b%2b&amp;source=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f&title=Starting%20out%20with%20CUDA%20C%2b%2b">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on whatsapp"
            href="https://api.whatsapp.com/send?text=Starting%20out%20with%20CUDA%20C%2b%2b%20-%20https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on telegram"
            href="https://telegram.me/share/url?text=Starting%20out%20with%20CUDA%20C%2b%2b&amp;url=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Starting out with CUDA C&#43;&#43; on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Starting%20out%20with%20CUDA%20C%2b%2b&u=https%3a%2f%2fnabergoj.org%2fposts%2fcuda_intro%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© 2025 - David Nabergoj</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
